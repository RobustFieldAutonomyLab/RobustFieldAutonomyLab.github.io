<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="1894.4">
  <style type="text/css">
    p.p6 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 13.0px Arial; color: #0f1a0e; -webkit-text-stroke: #0f1a0e}
    p.p7 {margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 18.0px; font: 13.0px Arial; color: #1a1a1a; -webkit-text-stroke: #1a1a1a}
    p.p8 {margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 12.0px; font: 9.0px Arial; color: #1a1a1a; -webkit-text-stroke: #1a1a1a}
    p.p9 {margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 18.0px; font: 13.0px Arial; color: #1a1a1a; -webkit-text-stroke: #1a1a1a; min-height: 15.0px}
    p.p10 {margin: 0.0px 0.0px 0.0px 0.0px; text-align: center; line-height: 18.0px; font: 13.0px Arial; color: #0f1a0e; -webkit-text-stroke: #0f1a0e}
    p.p11 {margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 18.0px; font: 13.0px Arial; color: #0f1a0e; -webkit-text-stroke: #0f1a0e; min-height: 15.0px}
    p.p13 {margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 18.0px; font: 13.0px Arial; color: #0f1a0e; -webkit-text-stroke: #0f1a0e}
    p.p14 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 13.0px Arial; color: #0f1a0e; -webkit-text-stroke: #0f1a0e; min-height: 15.0px}
    p.p15 {margin: 0.0px 0.0px 0.0px 0.0px; text-align: center; line-height: 80.0px; font: 13.0px Arial; color: #ffffff; -webkit-text-stroke: #ffffff}
    li.li3 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 40.0px; font: 12.0px Arial; color: #ffffff; -webkit-text-stroke: #ffffff}
    li.li4 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 40.0px; font: 13.0px Arial; color: #0f1a0e}
    span.s1 {font-kerning: none}
    span.s2 {font-kerning: none; color: #fdab5c; -webkit-text-stroke: 0px #fdab5c}
    span.s3 {-webkit-text-stroke: 0px #000000}
    span.s4 {font-kerning: none; -webkit-text-stroke: 0px #0f1a0e}
    span.s5 {font: 13.0px Arial; font-kerning: none; color: #303a58; -webkit-text-stroke: 0px #303a58}
    span.s6 {font: 13.0px Arial; font-kerning: none; color: #ffffff; -webkit-text-stroke: 0px #ffffff}
    ul.ul1 {list-style-type: none}
  </style>
</head>
<body>
<h1 style="margin: 0.0px 0.0px 0.0px 0.0px; line-height: 27.0px; font: 29.0px Arial; color: #000000; -webkit-text-stroke: #000000"><span class="s1">The Robust Field Autonomy Lab at Stevens Institute of Technology</span><span class="s2"> </span></h1>
<h1 style="margin: 0.0px 0.0px 0.0px 0.0px; line-height: 27.0px; font: 24.2px Arial; color: #000000; -webkit-text-stroke: #000000"><span class="s1">Research Group of Brendan Englot, Ph.D. </span></h1>
<ul class="ul1">
  <li class="li3"><span class="s3"><a href="file:///Users/brendanenglot/Documents/RFAL_Website/public_html/index.html"><span class="s1">Home</span></a></span><span class="s1"><br>
</span></li>
  <li class="li3"><span class="s3"><a href="file:///Users/brendanenglot/Documents/RFAL_Website/public_html/people.html"><span class="s1">People</span></a></span><span class="s1"><br>
</span></li>
  <li class="li4"><span class="s4"><br>
</span></li>
  <li class="li3"><span class="s3"><a href="file:///Users/brendanenglot/Documents/RFAL_Website/public_html/publications.html"><span class="s1">Publications</span></a></span><span class="s1"><br>
</span></li>
  <li class="li4"><span class="s4"><br>
</span></li>
</ul>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">Robust Autonomy in Complex Environments</span></h2>
<p class="p6"><span class="s1"><img src="file:///Hull_Inspection.png" alt="Hull_Inspection.png">    </span></p>
<p class="p7"><span class="s1">We design algorithms, optimization methods, and control systems that help mobile robots achieve robust autonomy in complex physical environments. Specific goals include improving the reliability of autonomous navigation for unmanned underwater, ground and aerial vehicles subjected to noise-corrupted and drifting sensors, incomplete knowledge of the environment, and tasks that require interaction with surrounding objects and structures. Recent work has considered sensing tasks motivated by underwater surveillance and inspection applications, autonomous exploration under sparse and noisy data, and path planning with multiple objectives, unreliable sensors, and imprecise maps.</span></p>
<p class="p6"><span class="s1"> <img src="file:///VideoRay1.png" alt="VideoRay1.png"> <img src="file:///VideoRay3.png" alt="VideoRay3.png"> <img src="file:///VideoRay2.png" alt="VideoRay2.png"></span></p>
<p class="p8"><span class="s1"><br>
</span></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"><img src="file:///Jackal_Photo_4.png" alt="Jackal_Photo_4.png"> <img src="file:///Jackal_Photo_2.jpg" alt="Jackal_Photo_2.jpg"> <img src="file:///Jackal_Photo_3.png" alt="Jackal_Photo_3.png"></span></p>
<p class="p7"><span class="s1"><b>Top:</b> Lab members complete a training session with our underwater robot, the VideoRay Pro4 Remotely-Operated Vehicle. <b>Bottom:</b> Bench-testing our Clearpath Jackal unmanned ground vehicle after a field experiment in Hoboken's Pier A Park.</span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">Recent News:</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">Sonar-Based Detection and Tracking of Underwater Pipelines</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"> <span class="Apple-converted-space"> </span></span></p>
<p class="p7"><span class="s1">At ICRA 2019's <a href="http://icra-2019-uwroboticsperception.ge.issia.cnr.it/2019-04-17-acceptedpapers/"><span class="s5"><b>Underwater Robotics Perception Workshop</b></span></a>, we recently presented our work on deep learning-enabled detection and tracking of underwater pipelines using multibeam imaging sonar, which is collaborative research with our colleagues at Schlumberger. In the above video, our BlueROV performs an automated flyover of a pipeline placed in Stevens' Davidson Laboratory towing tank.</span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">Learning-Aided Terrain Mapping Code Release</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"> <span class="Apple-converted-space"> </span></span></p>
<p class="p7"><span class="s1">We have developed a terrain mapping algorithm that uses Bayesian generalized kernel (BGK) inference for accurate traversability mapping under sparse Lidar data. The BGK terrain mapping algorithm was presented at the <a href="http://proceedings.mlr.press/v87/shan18a"><span class="s5"><b>2nd Annual Conference on Robot Learning</b></span></a>. We encourage you to download our library from <a href="https://github.com/RobustFieldAutonomyLab/BGK_traversability_mapping"><span class="s5"><b>GitHub</b></span></a>. A specialized version for ROS supported unmanned ground vehicles, which includes Lidar odometry and motion planning, is also available on <a href="https://github.com/RobustFieldAutonomyLab/traversability_mapping"><span class="s5"><b>GitHub</b></span></a>. The author and maintainer of both libraries is Tixiao Shan.<span class="Apple-converted-space"> </span></span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">Marine Robotics Research Profiled by NJTV News</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"> <span class="Apple-converted-space"> </span></span></p>
<p class="p7"><span class="s1">NJTV News recently joined us for a laboratory experiment with our BlueROV underwater robot where we tested its ability to autonomously track an underwater pipeline using deep learning-enabled segmentation of its sonar imagery. The full article describing how this work may aid the inspection of New Jersey's infrastructure is available at <a href="https://www.njtvonline.org/news/video/how-machine-learning-can-help-support-new-jerseys-infrastructure/"><span class="s5"><b>NJTV News</b></span></a>.<span class="Apple-converted-space"> </span></span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">LeGO-LOAM: Lightweight, Ground-Optimized Lidar Odometry and Mapping</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"> <span class="Apple-converted-space"> </span></span></p>
<p class="p7"><span class="s1">We have developed a new Lidar odometry and mapping algorithm intended for ground vehicles, which uses small quantities of features and is suitable for computationally lightweight, embedded systems applications. Ground-based and above-ground features are used to solve different components of the six degree-of-freedom transformation between consecutive Lidar frames. The algorithm was presented earlier this year at the University of Minnesota's <a href="http://www.roadwaysafety.umn.edu/events/seminars/2018/012518/"><span class="s5"><b>Roadway Safety Institute Seminar Series</b></span></a>. We are excited that LeGO-LOAM will appear at IROS 2018! We encourage you to download our library from <a href="https://github.com/RobustFieldAutonomyLab/LeGO-LOAM"><span class="s5"><b>GitHub</b></span></a>. The author and maintainer of this library is Tixiao Shan.<span class="Apple-converted-space"> </span></span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">3D Mapping Code Release - The Learning-Aided 3D Mapping Library (LA3DM)</span></h2>
<p class="p10"><span class="s1"><img src="file:///Mapping_Overview.jpg" alt="Mapping_Overview.jpg"></span></p>
<p class="p13"><span class="s1">We have released our <a href="https://github.com/RobustFieldAutonomyLab/la3dm"><span class="s5"><b>Learning-Aided 3D Mapping (LA3DM) Library</b></span></a>, which includes our implementations of Gaussian process occupancy mapping (GPOctoMap - Wang and Englot, ICRA 2016) and Bayesian generalized kernel occupancy mapping (BGKOctoMap - Doherty, Wang and Englot, ICRA 2017). We encourage you to download our library from <a href="https://github.com/RobustFieldAutonomyLab/la3dm"><span class="s5"><b>GitHub</b></span></a>. The authors and maintainers of this library are Jinkun Wang and Kevin Doherty.</span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1"><br>
</span></h2>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">Autonomous Navigation with Jackal UGV</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"> <span class="Apple-converted-space"> </span></span></p>
<p class="p7"><span class="s1">We have developed terrain traversability mapping and autonomous navigation capability for our LIDAR-equipped Clearpath Jackal Unmanned Ground Vehicle (UGV). This work by Tixiao Shan was recently highlighted on the <a href="https://www.clearpathrobotics.com/sit-advances-autonomous-mapping-navigation-research-using-jackal-ugv/"><span class="s5"><b>Clearpath Robotics Blog</b></span></a>.</span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">ROS Package for 3D Mapping with a Hokuyo UTM-30LX Laser Rangefinder</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"><img src="file:///rotating_hokuyo_composite.jpg" alt="rotating_hokuyo_composite.jpg"></span></p>
<p class="p11"><span class="s1"></span><br></p>
<p class="p13"><span class="s1">We have released a new ROS package to produce 3D point clouds using a Hokuyo UTM-30LX scanning laser rangefinder and a Dynamixel MX-28 servo. Please visit the ROS wiki page for the package <a href="http://wiki.ros.org/spin_hokuyo"><span class="s5"><b>spin_hokuyo</b></span></a> for more information on how to download, install, and run our software. The authors and maintainers of this package are Sarah Bertussi and Paul Szenher.</span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1"><br>
</span></h2>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">3D Exploration ROS Package for Turtlebot</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"> <span class="Apple-converted-space"> </span></span></p>
<p class="p7"><span class="s1">We have released a 3D autonomous exploration ROS package for the TurtleBot! Please visit the ROS wiki page for the package <a href="http://wiki.ros.org/turtlebot_exploration_3d"><span class="s5"><b>turtlebot_exploration_3d</b></span></a> for more information on how to download, install, and run our software. The authors and maintainers of this package are Xiangyu Xu and Shi Bai. </span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">Recent Underwater Localization and 3D Mapping Results</span></h2>
<p class="p11"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"> <span class="Apple-converted-space"> </span></span></p>
<p class="p7"><span class="s1">We recently visited Pier 84 in Manhattan to test our algorithms for underwater localization and 3D mapping, supported by a single-beam scanning sonar. See above for a summary of our results from this field experiment, which is detailed in the ICRA 2017 paper "Underwater Localization and 3D Mapping of Submerged Structures with a Single-Beam Scanning Sonar," by Jinkun Wang, Shi Bai, and Brendan Englot.</span></p>
<h2 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 30.0px; font: 19.5px Arial; color: #303a58; -webkit-text-stroke: #303a58"><span class="s1">Moved to a New Laboratory Facility</span></h2>
<p class="p10"><span class="s1"><img src="file:///ABS_lab_photo.png" alt="ABS_lab_photo.png"> <img src="file:///ABS_grand_opening.png" alt="ABS_grand_opening.png"> <span class="Apple-converted-space"> </span></span></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p7"><span class="s1"><br>
</span></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p7"><span class="s1"><br>
</span></p>
<p class="p7"><span class="s1">Our lab recently relocated to the ABS Engineering Center, a newly renovated facility at Stevens that will support interdisciplinary research and education in civil, mechanical, and naval engineering. Our lab sits in the former location of Tank 2, a 75' square rotating arm basin that was built in 1942, whose walls still form the perimeter of the facility. </span></p>
<p class="p13"><span class="s1"><b>At top:</b> A photo of the ABS Engineering Center, with the entrance to the Robust Field Autonomy Lab at bottom center. The former rotating arm of Tank 2 is visible at top.</span></p>
<p class="p7"><span class="s1"><b>At bottom:</b> Members of the lab at the ABS Engineering Center's grand opening in November 2016.</span></p>
<p class="p14"><span class="s1"></span><br></p>
<p class="p10"><span class="s1"><img src="file:///stevens_logo.png" alt="stevens_logo.png"></span></p>
<p class="p15"><span class="s1">© Copyright 2019 <a href="http://web.stevens.edu/"><span class="s6">Stevens Institute of Technology</span></a> | <a href="http://web.stevens.edu/ses"><span class="s6">School of Engineering and Science Home</span></a> | <a href="http://www.stevens.edu/schaefer-school-engineering-science/departments/mechanical-engineering"><span class="s6">Mechanical Engineering Home</span></a></span></p>
</body>
</html>
