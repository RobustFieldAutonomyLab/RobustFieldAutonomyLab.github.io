<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" /><title>Robust Field Autonomy Lab</title>

</head>
<body>
<div id="wrap">
<div style="font-family: Calibri;" id="header">
<h1><span style="color: black;">The&nbsp;Robust
Field Autonomy Lab at Stevens Institute
of Technology</span>&nbsp;</h1>
<h1 style="color: black;"><small>Research Group of
Brendan Englot, Ph.D.&nbsp;</small></h1>
</div>
<div id="menu">
<ul>
<li><a href="index.html">Home</a></li>
<li><a href="people.html">People</a></li>
<li><br />
</li>
<li><a href="publications.html">Publications</a></li>
<li><br />
</li>
</ul>
</div>
<div id="contentwrap">
<div style="width: 745px;" id="content">
<h2>Robust Autonomy in Complex Environments</h2>
<div style="padding: 10px 0pt; float: left;"><img style="width: 285px; height: 194px;" src="./index_files/Hull_Inspection.png" alt="image" /> &nbsp; &nbsp;<br />
</div>
<p style="text-align: justify;">We design<span>
perception and decision-making algorithms, optimization methods, and control systems that help mobile
robots&nbsp;achieve robust autonomy in complex physical
environments. Specific goals include improving the reliability of
autonomous navigation for unmanned underwater, ground and aerial
vehicles
subjected to noise-corrupted and drifting sensors, incomplete knowledge
of the environment, and tasks that require interaction with surrounding
objects and structures. Recent work has considered sensing tasks
motivated by underwater surveillance and inspection applications,
autonomous exploration under sparse and noisy data, and path planning
with multiple objectives, unreliable sensors, and imprecise maps.</span></p>
<br />
<div style="text-align: center;"> <img style="width: 251px; height: 131px;" alt="" src="./index_files/VideoRay1.jpg" />&nbsp;<img style="width: 210px; height: 131px;" alt="" src="./index_files/VideoRay3.jpg" /> <img style="width: 258px; height: 131px;" alt="" src="./index_files/VideoRay2.jpg" /><br />
</div>
<div style="text-align: center;"> <img style="width: 155px; height: 163px;" alt="" src="./index_files/BlueROV.jpg" />&nbsp;<img style="width: 262px; height: 163px;" alt="" src="./index_files/rov_pier_test.jpg" /> <img style="width: 197px; height: 163px;" alt="" src="./index_files/sonar_point_cloud.png" /><br />
</div>
<div style="text-align: center;"> <img style="width: 168px; height: 133px;" alt="" src="./index_files/jackal_hilltop_2.jpg" />&nbsp;<img style="width: 238px; height: 133px;" alt="" src="./index_files/Jackal_Photo_2.jpg" />&nbsp;<img style="width: 237px; height: 133px;" alt="" src="./index_files/Jackal_Photo_3.png" /><br />
</div>
<p style="text-align: justify;"><span style="font-weight: bold;">Top:</span> Lab members
complete a training session with the VideoRay
Pro4, one of our lab's three Remotely Operated Vehicles. <br>
<span style="font-weight: bold;">Center:</span>
Acoustically mapping the pilings of Stevens' Hudson River pier with our custom-built BlueROV. <br>
<span style="font-weight: bold;">Bottom:</span>
Bench-testing our Clearpath Jackal unmanned ground vehicle after a
field experiment in Hoboken's Pier A Park.</p>
<h2>Recent News:</h2>
<div style="text-align: justify;">
<br />
<h2>Zero-Shot Reinforcement Learning on Graphs for Autonomous Exploration
</h2><br /> 
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/62phOSf2HEg" allowfullscreen="" frameborder="0" height="315" width="560"></iframe>
</div>
<p style="text-align: justify;">We are excited that our paper "Zero-Shot Reinforcement Learning on Graphs for Autonomous Exploration Under Uncertainty" has been accepted for presentation at ICRA 2021.
In the video above, which assumes a lidar-equipped mobile robot depends on segmentation-based SLAM for localization, we show the exploration policy learned
by training in a single Gazebo environment, and its successful transfer both to other virtual environments and to robot hardware.
A preprint of the paper is available on <a style="font-weight: bold;" href="https://arxiv.org/pdf/2105.04758.pdf">arXiv</a>, our presentation
of the paper can be viewed <a style="font-weight: bold;" href="https://robustfieldautonomylab.github.io/Chen_ICRA_2021_Presentation.mp4">here</a>. This work was led by Fanfei Chen.  
</p>
<h2>Predictive Large-Scale 3D Underwater Mapping with Sonar
</h2><br /> 
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/WouCrY9eK4o" allowfullscreen="" frameborder="0" height="315" width="560"></iframe>
</div>
<p style="text-align: justify;">We are pleased to announce that our paper on predictive large-scale 3D underwater mapping using a pair of wide-aperture imaging sonars has been accepted for presentation at ICRA 2021.
This work features our custom-built heavy configuration BlueROV underwater robot, which is equipped with two orthongally oriented Oculus multibeam sonars (the software packages
 for our BlueROV can be found on <a style="font-weight: bold;" href="https://github.com/jake3991/Argonaut">GitHub</a>).
A preprint of the paper is available on <a style="font-weight: bold;" href="https://arxiv.org/pdf/2104.03203.pdf">arXiv</a>, and our presentation
 of the paper can be viewed <a style="font-weight: bold;" href="https://robustfieldautonomylab.github.io/McConnell_ICRA_2021_Presentation.mp4">here</a>. This work was led by John McConnell.  
</p>
<h2>Lidar Super-resolution Paper and Code Release</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/rNVTpkz2ggY" allowfullscreen="" frameborder="0" height="315" width="560"></iframe>
<br />
</div>
<p style="text-align: justify;">We have developed a
framework for lidar super-resolution that is trained completely using synthetic data from the <a style="font-weight: bold;" href="https://www.carla.org">CARLA Urban Driving Simulator</a>. It is capable of accurately enhancing
 the apparent resolution of a physical lidar across a wide variety of real-world environments. Our paper on this work was recently published in <a style="font-weight: bold;" href="https://www.sciencedirect.com/science/article/pii/S0921889020304875">Robotics and Autonomous Systems</a>, and we encourage you to download
our library from <a style="font-weight: bold;" href="https://github.com/RobustFieldAutonomyLab/lidar_super_resolution">GitHub</a>.
The author and maintainer of this library is Tixiao Shan.
</p>
<h2>Copula Models for Capturing Probabilistic Dependencies in SLAM</h2>
<br />
<div style="text-align: center;"><img style="width: 400px; height: 400px;" alt="" src="./index_files/factor_graph_copulas.png" /><br />
</div>
<p style="text-align: justify;">We are happy to announce that our paper on using copulas for modeling the probabilistic dependencies in simultaneous localization and mapping (SLAM) with landmarks has been accepted for 
presentation at IROS 2020. A preprint of the paper "Variational Filtering with Copula Models for SLAM" is available on <a style="font-weight: bold;" href="https://arxiv.org/pdf/2008.00504.pdf">arXiv</a> and our presentation
 of the paper can be viewed <a style="font-weight: bold;" href="https://robustfieldautonomylab.github.io/index_files/Martin_IROS_2020_Presentation.mp4">here</a>.
This collaborative work with MIT was led jointly by John Martin and lab alumnus Kevin Doherty. 
</p>
<h2>Autonomous Exploration using Deep Reinforcement Learning on Graphs 
</h2><br /> 
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/e7uM03hMZRo" allowfullscreen="" frameborder="0" height="315" width="560"></iframe>
</div>
<p style="text-align: justify;">We are pleased to announce that our paper "Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs" has been accepted for presentation at IROS 2020.
In the video above, which assumes a range-sensing mobile robot depends on the observation of point landmarks for localization, we show the performance of several 
competing architectures that combine deep RL with graph neural networks to learn how to efficiently explore
unknown environments, while building accurate maps.
A preprint of the paper is available on <a style="font-weight: bold;" href="https://arxiv.org/pdf/2007.12640.pdf">arXiv</a>, our presentation
of the paper can be viewed <a style="font-weight: bold;" href="https://robustfieldautonomylab.github.io/index_files/Chen_IROS_2020_Presentation.mp4">here</a>, and we encourage you to download
our code from <a style="font-weight: bold;" href="https://github.com/RobustFieldAutonomyLab/DRL_graph_exploration">GitHub</a>. This work was led by Fanfei Chen, who is
the author and maintainer of the "DRL Graph Exploration" library.  
</p>
<h2>Dense Underwater 3D Reconstruction with a Pair of Wide-aperture Imaging Sonars 
</h2><br /> 
<div style="text-align: center;"><video width="560" height="315" controls> <source src="./McConnell_IROS_2020_video.mp4" type=video/mp4> </video><br />
</div><br /> 
We are pleased to announce that our paper on dense underwater 3D reconstruction using a pair of wide-aperture imaging sonars has been accepted for presentation at IROS 2020.
This work features our custom-built heavy configuration BlueROV underwater robot, which is equipped with two orthongally oriented Oculus multibeam sonars (the software packages
 for our BlueROV can be found on <a style="font-weight: bold;" href="https://github.com/jake3991/Argonaut">GitHub</a>).
A preprint of the paper is available on <a style="font-weight: bold;" href="https://arxiv.org/pdf/2007.10407.pdf">arXiv</a>, and our presentation
 of the paper can be viewed <a style="font-weight: bold;" href="https://robustfieldautonomylab.github.io/index_files/McConnell_IROS_2020_Presentation.mp4">here</a>. This work was led by John McConnell.  
<h2><br />
<h2>Lidar Inertial Odometry via Smoothing and Mapping (LIO-SAM)</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/OF_wOgPTNhs" allowfullscreen="" frameborder="0" height="315" width="560"></iframe>
</div>
<p style="text-align: justify;">We recently brought our Jackal UGV to a nearby park to perform some additional validation of LIO-SAM, a framework for tightly-coupled lidar inertial odometry which will be presented at
IROS 2020. A preprint of the paper is available on <a style="font-weight: bold;" href="https://arxiv.org/pdf/2007.00258v3.pdf">arXiv</a>, a presentation
of the paper can be viewed <a style="font-weight: bold;" href="https://robustfieldautonomylab.github.io/index_files/Shan_IROS_2020_Presentation.mp4">here</a>, and we encourage you to download
the library from <a style="font-weight: bold;" href="https://github.com/TixiaoShan/LIO-SAM">GitHub</a>.
This collaborative work with MIT was led by lab alumnus Dr. Tixiao Shan, who is the author and maintainer of the LIO-SAM library. 
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/lheEmUZwBzU" allowfullscreen="" frameborder="0" height="315" width="560"></iframe>
</div>
<p style="text-align: justify;">We also recently mounted the new 128-beam Ouster OS1-128 lidar on our Jackal UGV, and performed some additional LIO-SAM mapping on the Stevens campus (all earlier results
 have been gathered using the 16-beam Velodyne VLP-16). It was encouraging to see LIO-SAM support real-time operation despite the greatly-increased sensor resolution. 
</p>
<h2>Stochastically Dominant Distributional Reinforcement Learning</h2>
<br />
<div style="text-align: center;"><img style="width: 654px; height: 300px;" alt="" src="./index_files/ssd_drl_overview_image.png" /><br />
</div>
<p style="text-align: justify;">We are happy to announce that our paper on risk-aware action selection in distributional reinforcement learning has been accepted for 
presentation at the 2020 International Conference on Machine Learning (ICML). A preprint of the paper "Stochastically Dominant Distributional Reinforcement Learning" is available on <a style="font-weight: bold;" href="https://arxiv.org/pdf/1905.07318.pdf">arXiv</a>, and our presentation
of the paper can be viewed <a style="font-weight: bold;" href="https://icml.cc/virtual/2020/poster/6410">here</a>.
This work was led by John Martin. 
</p>
<h2>Active Perception with the BlueROV Underwater Robot 
</h2><br /> 
<div style="text-align: center;"><video width="560" height="315" controls> <source src="./index_files/em_3.mp4" type=video/mp4> </video><br />
</div><br /> 
We have recently adapted our algorithms for Expectation-Maximization based autonomous mobile robot exploration published at ISRR (Wang and Englot, ISRR 2017)
and IROS (Wang, Shan and Englot, IROS 2019) to an underwater active SLAM setting with our BlueROV underwater robot, which uses its imaging sonar for SLAM. This work was performed by Jinkun Wang
 with the help of Tixiao Shan and John McConnell, and more results will be forthcoming soon. A recent seminar discussing our work on this topic can be viewed <a style="font-weight: bold;" href="https://kaltura.stevens.edu/media/%22Virtual+Maps+for+High-Performance+Mobile+Robot+Exploration+Under+Uncertainty%22+with+Professor+Brendan+Englot/1_ijlmuwgi">here</a>.
<h2><br />  
<h2>Sonar-Based Detection and Tracking of Underwater Pipelines</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/oL45QrxqbYI" allowfullscreen="" frameborder="0" height="315" width="560"></iframe> <br />
</div>
<p style="text-align: justify;">At ICRA 2019's <a style="font-weight: bold;" href="http://icra-2019-uwroboticsperception.ge.issia.cnr.it/2019-04-17-acceptedpapers/">Underwater Robotics Perception Workshop</a>,
we recently presented our work
on deep learning-enabled detection and tracking of underwater pipelines
using multibeam imaging sonar, which is collaborative
research with our colleagues at Schlumberger. In the above video, our
BlueROV performs an automated flyover of a pipeline placed in Stevens'
Davidson Laboratory towing tank. Our paper describing this work is available <a style="font-weight: bold;" href="http://personal.stevens.edu/~benglot/Wang_ICRA_2019_UWPerceptionWorkshop.pdf">here</a>.<span></span></p>
<h2>Learning-Aided Terrain Mapping Code Release</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/4pdBpeRGXmw" allowfullscreen="" frameborder="0" height="315" width="560"></iframe>
<br />
</div>
<p style="text-align: justify;">We have developed a
terrain mapping
algorithm that uses Bayesian generalized kernel (BGK) inference for
accurate traversability
mapping under sparse Lidar data. The BGK terrain mapping
algorithm was presented at the <a style="font-weight: bold;" href="http://proceedings.mlr.press/v87/shan18a">2nd Annual
Conference on Robot Learning</a>. We encourage you to download
our
library from <a style="font-weight: bold;" href="https://github.com/RobustFieldAutonomyLab/BGK_traversability_mapping">GitHub</a>.
A specialized version for ROS supported unmanned ground vehicles, which
includes Lidar odometry and motion planning, is also available on <a style="font-weight: bold;" href="https://github.com/RobustFieldAutonomyLab/traversability_mapping">GitHub</a>.
The author and maintainer of both libraries is Tixiao Shan.
</p>
<h2>Marine Robotics Research Profiled by NJTV News</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://player.pbs.org/viralplayer/3015116434/" marginwidth="0" marginheight="0" seamless="" allowfullscreen="" frameborder="0" height="332" scrolling="no" width="512"></iframe>
<br />
</div>
<p style="text-align: justify;">NJTV News recently joined
us for a
laboratory experiment with our BlueROV underwater robot where we tested
its ability to autonomously track an underwater pipeline using deep
learning-enabled segmentation of its sonar imagery. The full article
describing how this work may aid the inspection of New Jersey's
infrastructure is available at <a style="font-weight: bold;" href="https://www.njtvonline.org/news/video/how-machine-learning-can-help-support-new-jerseys-infrastructure/">NJTV
News</a>. </p>
<h2>LeGO-LOAM: Lightweight, Ground-Optimized Lidar Odometry and
Mapping</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/O3tz_ftHV48" allowfullscreen="" frameborder="0" height="315" width="560"></iframe>
<br />
</div>
<p style="text-align: justify;">We have developed a new
Lidar odometry
and mapping algorithm intended for ground vehicles, which uses small
quantities of features and is suitable for computationally
lightweight, embedded systems applications. Ground-based and
above-ground features are used to solve different components of the six
degree-of-freedom transformation between consecutive Lidar frames. The
algorithm was presented earlier this year at the University of
Minnesota's <a style="font-weight: bold;" href="http://www.roadwaysafety.umn.edu/events/seminars/2018/012518/">Roadway
Safety Institute Seminar Series</a>. We are excited that
LeGO-LOAM will appear at IROS 2018! We encourage you to download our
library from <a style="font-weight: bold;" href="https://github.com/RobustFieldAutonomyLab/LeGO-LOAM">GitHub</a>.
The author and maintainer of this library is Tixiao Shan. </p>
<h2>3D Mapping Code Release - The Learning-Aided 3D Mapping
Library (LA3DM)</h2>
<div style="text-align: center;"><img style="width: 654px; height: 367px;" alt="" src="./index_files/Mapping_Overview.jpg" /><br />
</div>
We have released our <a style="font-weight: bold;" href="https://github.com/RobustFieldAutonomyLab/la3dm">Learning-Aided
3D Mapping (LA3DM) Library</a>,
which includes our implementations of Gaussian process occupancy
mapping (GPOctoMap - Wang and Englot, ICRA 2016) and Bayesian
generalized kernel occupancy mapping (BGKOctoMap - Doherty, Wang and
Englot, ICRA 2017). We encourage you to download our library from <a style="font-weight: bold;" href="https://github.com/RobustFieldAutonomyLab/la3dm">GitHub</a>.
The authors and maintainers of this library are Jinkun Wang and Kevin
Doherty.
<h2><br />
</h2>
<h2>Autonomous Navigation with Jackal UGV</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/B6lrbAEhEnE" allowfullscreen="" frameborder="0" height="315" width="560"></iframe> <br />
</div>
<p style="text-align: justify;">We have developed terrain
traversability mapping and autonomous navigation capability for our
LIDAR-equipped Clearpath Jackal Unmanned Ground Vehicle (UGV). This
work by Tixiao Shan was recently highlighted on the <a style="font-weight: bold;" href="https://www.clearpathrobotics.com/sit-advances-autonomous-mapping-navigation-research-using-jackal-ugv/">Clearpath
Robotics Blog</a>.</p>
<h2>ROS Package for 3D Mapping with a Hokuyo UTM-30LX Laser
Rangefinder</h2>
<br />
<div style="text-align: center;"><img style="width: 654px; height: 275px;" alt="" src="./index_files/rotating_hokuyo_composite.jpg" /><br />
</div>
<br />
We have released a new ROS package to produce 3D point clouds using a
Hokuyo UTM-30LX scanning laser rangefinder and a Dynamixel MX-28 servo.
Please visit the ROS wiki page for the package&nbsp;<a style="font-weight: bold;" href="http://wiki.ros.org/spin_hokuyo">spin_hokuyo</a>
for more information on how to download, install, and run our software.
The authors and maintainers of this package are Sarah Bertussi and Paul
Szenher.
<h2><br />
</h2>
<h2>3D Exploration ROS Package for Turtlebot</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/ocZOaySmwHU" allowfullscreen="" frameborder="0" height="315" width="560"></iframe> <br />
</div>
<p style="text-align: justify;">We have released a 3D
autonomous exploration ROS package for the TurtleBot! Please visit the
ROS wiki page for the package <a style="font-weight: bold;" href="http://wiki.ros.org/turtlebot_exploration_3d">turtlebot_exploration_3d</a>
for more information on how to download, install, and run our software.
The authors and maintainers of this package are Xiangyu Xu and Shi
Bai.&nbsp;</p>
<h2>Recent Underwater Localization and 3D Mapping Results</h2>
<br />
<div style="text-align: center;">&nbsp;<iframe src="https://www.youtube.com/embed/XdkxnGSEufw" allowfullscreen="" frameborder="0" height="315" width="560"></iframe> <br />
</div>
<p style="text-align: justify;">We recently visited Pier
84 in
Manhattan to test our algorithms for underwater localization and 3D
mapping, supported by a single-beam scanning sonar. See above for a
summary of our results from this field experiment, which is detailed in
the ICRA 2017 paper "Underwater Localization and 3D Mapping of
Submerged Structures with a Single-Beam Scanning Sonar," by Jinkun
Wang, Shi Bai, and Brendan Englot.<span></span></p>
<h2>Moved to a New Laboratory Facility</h2>
<div style="padding: 10px 0pt; float: left; text-align: center;"><img style="width: 604px; height: 395px;" alt="" src="./index_files/ABS_lab_photo.png" />&nbsp;<img style="width: 603px; height: 399px;" alt="" src="./index_files/ABS_grand_opening.png" />&nbsp;
</div>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"><br />
</p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"></p>
<p style="text-align: justify;"><br />
</p>
<p style="text-align: justify;">Our lab recently relocated
to the ABS Engineering Center, a newly renovated facility at Stevens
that will support interdisciplinary research and education in civil,
mechanical, and naval engineering. Our lab sits in the former location
of&nbsp;Tank 2, a 75' square rotating arm basin that was built in
1942,
whose walls still form the perimeter of the facility.&nbsp;</p>
<span style="font-weight: bold;">At top:</span> A
photo of the ABS Engineering Center, with the entrance to the Robust
Field Autonomy Lab at bottom center. The former rotating arm
of&nbsp;Tank
2 is visible at top.
<p style="text-align: justify;"><span style="font-weight: bold;">At bottom:</span> Members
of the lab at the ABS Engineering Center's grand opening in November
2016.</p>
</div>
<br />
</div>
<div style="clear: both; text-align: center;"><img alt="" style="width: 299px; height: 127px;" src="./index_files/stevens_logo.png" /></div>
</div>
<div id="footer">
<p>© Copyright 2021 <a href="http://web.stevens.edu">Stevens
Institute of Technology</a> | <a href="http://web.stevens.edu/ses">School of Engineering and
Science Home</a> | <a href="http://www.stevens.edu/schaefer-school-engineering-science/departments/mechanical-engineering">Mechanical
Engineering Home</a></p>
</div>
</div>
</body></html>
